{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "53e9300e-6211-4a6d-9e4c-01b81eef9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5_getter\n",
    "import numpy as np\n",
    "import os\n",
    "import pypianoroll\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import music21\n",
    "import random\n",
    "import json\n",
    "import itertools\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c530586-1b01-4793-8560-5f6aefcf38f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parsh\\AppData\\Local\\Temp\\ipykernel_16240\\2824692179.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  cleansed_ids = pd.read_csv(os.path.join('dataset', 'cleansed_ids.txt'), delimiter = '    ', header = None)\n"
     ]
    }
   ],
   "source": [
    "root_dir = '.'\n",
    "data_dir = root_dir + '/dataset/lpd_5_cleansed'\n",
    "music_dataset_lpd_dir = root_dir + '/dataset/lmd_matched_h5'\n",
    "\n",
    "cleansed_ids = pd.read_csv(os.path.join('dataset', 'cleansed_ids.txt'), delimiter = '    ', header = None)\n",
    "lpd_to_msd_ids = {a:b for a, b in zip(cleansed_ids[0], cleansed_ids[1])}\n",
    "msd_to_lpd_ids = {a:b for a, b in zip(cleansed_ids[1], cleansed_ids[0])}\n",
    "\n",
    "RESULTS_PATH = os.path.join(root_dir, 'dataset')\n",
    "\n",
    "# Utility functions for retrieving paths\n",
    "def msd_id_to_dirs(msd_id):\n",
    "    \"\"\"Given an MSD ID, generate the path prefix.\n",
    "    E.g. TRABCD12345678 -> A/B/C/TRABCD12345678\"\"\"\n",
    "    return os.path.join(msd_id[2], msd_id[3], msd_id[4], msd_id)\n",
    "\n",
    "\n",
    "def msd_id_to_h5(msd_id):\n",
    "    \"\"\"Given an MSD ID, return the path to the corresponding h5\"\"\"\n",
    "    return os.path.join(RESULTS_PATH, 'lmd_matched_h5',\n",
    "                        msd_id_to_dirs(msd_id) + '.h5')\n",
    "\n",
    "# Load the midi npz file from the LMD cleansed folder\n",
    "def get_midi_npz_path(msd_id, midi_md5):\n",
    "    return os.path.join(data_dir,\n",
    "                        msd_id_to_dirs(msd_id), midi_md5 + '.npz')\n",
    "    \n",
    "# Load the midi file from the Music Dataset folder\n",
    "def get_midi_path(msd_id, midi_md5):\n",
    "    return os.path.join(music_dataset_lpd_dir,\n",
    "                        msd_id_to_dirs(msd_id), midi_md5 + '.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ff220bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the genre annotations\n",
    "genre_file_dir = os.path.join('dataset', 'msd_tagtraum_cd1.cls')\n",
    "ids = []\n",
    "genres = []\n",
    "with open(genre_file_dir) as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        if line[0] != '#':\n",
    "          split = line.strip().split(\"\\t\")\n",
    "          if len(split) == 2:\n",
    "            ids.append(split[0])\n",
    "            genres.append(split[1])\n",
    "          elif len(split) == 3:\n",
    "            ids.append(split[0])\n",
    "            ids.append(split[0])\n",
    "            genres.append(split[1])\n",
    "            genres.append(split[2])\n",
    "        line = f.readline()\n",
    "genre_df = pd.DataFrame(data={\"TrackID\": ids, \"Genre\": genres})\n",
    "\n",
    "genre_dict = genre_df.groupby('TrackID')['Genre'].apply(lambda x: x.tolist()).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9b0a2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ids of pop songs\n",
    "pop_ids = genre_df[genre_df['Genre'] == 'Pop_Rock']['TrackID'].tolist()\n",
    "\n",
    "pop_lpd_ids = [msd_to_lpd_ids[msd_id] for msd_id in pop_ids if msd_id in msd_to_lpd_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44f53b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2423"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take random 200 pop songs\n",
    "pop_lpd_ids = np.random.choice(pop_lpd_ids, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ad940",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = []\n",
    "\n",
    "i = 0\n",
    "while i < 50:\n",
    "    lpd_file_name = pop_lpd_ids[np.random.randint(0, len(pop_lpd_ids) - 1)]\n",
    "    msd_file_name = lpd_to_msd_ids[lpd_file_name]\n",
    "\n",
    "    # Get the NPZ path\n",
    "    npz_path = get_midi_npz_path(msd_file_name, lpd_file_name)\n",
    "\n",
    "    multitrack = pypianoroll.load(npz_path)\n",
    "    pm = pypianoroll.to_pretty_midi(multitrack)\n",
    "    new_midi_path = npz_path[:-4] + '.mid'\n",
    "    pypianoroll.write(new_midi_path, multitrack)\n",
    "    # Get the MIDI path (should already be generated)\n",
    "    new_midi_path = npz_path[:-4] + '.mid'\n",
    "    midi = music21.converter.parse(new_midi_path)\n",
    "\n",
    "    s2 = music21.instrument.partitionByInstrument(midi)\n",
    "    piano_part = None\n",
    "    # Filter for  only the piano part\n",
    "    instr = music21.instrument.Piano\n",
    "    for part in s2:\n",
    "        if isinstance(part.getInstrument(), instr):\n",
    "            piano_part = part\n",
    "\n",
    "    notes_song = []\n",
    "    if piano_part: # Some songs somehow have no piano parts\n",
    "        for element in piano_part:\n",
    "            if isinstance(element, music21.note.Note):\n",
    "            # Return the pitch of the single note\n",
    "                notes_song.append(str(element.pitch))\n",
    "            elif isinstance(element, music21.chord.Chord):\n",
    "            # Returns the normal order of a Chord represented in a list of integers\n",
    "                notes_song.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    notes.append(notes_song)\n",
    "    i+=1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "78061cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_notes = [x for x in notes if len(x) > 0]\n",
    "len(filtered_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0731a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "test_ids = random.choices(list(range(480)), k = 48)\n",
    "train_ids = [e for e in range(480) if e not in test_ids]\n",
    "\n",
    "notes_train = [notes[i] for i in train_ids]\n",
    "notes_test = [notes[i] for i in test_ids]\n",
    "\n",
    "with open('train_notes.json', 'w') as f:\n",
    "    json.dump(notes_train, f)\n",
    "\n",
    "with open('test_notes.json', 'w') as f:\n",
    "    json.dump(notes_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "35b296d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input and output sequences\n",
    "def prepare_sequences(notes, note_to_int = None, sequence_length = 32):\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    if not note_to_int:\n",
    "        # Set of note/chords (collapse into list)\n",
    "        pitch_names = sorted(set(itertools.chain(*notes)))\n",
    "        # create a dictionary to map pitches to integers\n",
    "        note_to_int = dict((note, number) for number, note in enumerate(pitch_names))\n",
    "\n",
    "    # Loop through all songs\n",
    "    for song in notes:\n",
    "        # Check for the end\n",
    "        i = 0\n",
    "        while i + sequence_length < len(song):\n",
    "            # seq_len notes for the input seq\n",
    "            sequence_in = song[i: i + sequence_length]\n",
    "            # Next note to predict\n",
    "            sequence_out = song[i+sequence_length]\n",
    "            # Return the int representation of the note - *(If note not found)\n",
    "            network_input.append([note_to_int.get(char, 0) for char in sequence_in])\n",
    "            network_output.append(note_to_int.get(sequence_out, 0))\n",
    "            i += 5\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # Reshape for LSTM input\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # Normalize input (?? - CHECK LATER - this assumes the alphabetical order of the notes carries semantic meaning?)\n",
    "    #network_input = network_input / len(pitch_names)\n",
    "    #network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return network_input, network_output, note_to_int\n",
    "\n",
    "train_input, train_output, note_to_int = prepare_sequences(notes_train, sequence_length = 64)\n",
    "#test_input, test_output, _ = prepare_sequences(notes_test, note_to_int = note_to_int, sequence_length = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2f6c7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_note = {number:note for note, number in note_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b8047e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random observation from the network input, return (input, target), each shifted by 1\n",
    "# NOT NEEDED ANYMORE - each epoch just using entire dataset\n",
    "def random_training_set(network_input):    \n",
    "    chunk = network_input[random.randint(0, network_input.shape[0] - 1), : , :]\n",
    "    input = torch.tensor(chunk[:-1], dtype = torch.long).squeeze()\n",
    "    target = torch.tensor(chunk[1:], dtype = torch.long).squeeze()\n",
    "    return input, target\n",
    "\n",
    "\n",
    "def grad_clipping(net, theta):  \n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    \n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6cf757b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationRNN(nn.Module):\n",
    "  # input_size: number of possible pitches\n",
    "  # hidden_size: embedding size of each pitch\n",
    "  # output_size: number of possible pitches (probability distribution)\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(GenerationRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size * n_layers, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        # Creates embedding of the input texts\n",
    "        #print('initial input', input.size())\n",
    "        input = self.embedding(input.view(1, -1))\n",
    "        #print('input after embedding', input.size())\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        #print('output after gru', output.size())\n",
    "        #print('hidden after gru', hidden.size())\n",
    "        output = self.decoder(hidden.view(1, -1))\n",
    "        #print('output after decoder', output.size())\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "505ff347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single training step for ONE sequence\n",
    "def train_sequence(input, target, model, optimizer, criterion):\n",
    "    # Initialize hidden state, zero the gradients of model \n",
    "    hidden = model.init_hidden()\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    # For each character in our chunk (except last), compute the hidden and ouput\n",
    "    # Using each output, compute the loss with the corresponding target \n",
    "    for i in range(len(input)):\n",
    "        output, hidden = model(input[i], hidden)\n",
    "        loss += criterion(output, target[i].unsqueeze(0))\n",
    "    \n",
    "    # Backpropagate, clip gradient and optimize\n",
    "    loss.backward()\n",
    "    grad_clipping(model, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return average loss for the input sequence\n",
    "    return loss.data.item() / len(input)\n",
    "\n",
    "def test_sequence(input, target, model, criterion):\n",
    "    # Initialize hidden state, zero the gradients of model \n",
    "    hidden = model.init_hidden()\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    # For each character in our chunk (except last), compute the hidden and ouput\n",
    "    # Using each output, compute the loss with the corresponding target \n",
    "    for i in range(len(input)):\n",
    "        output, hidden = model(input[i], hidden)\n",
    "        loss += criterion(output, target[i].unsqueeze(0))\n",
    "\n",
    "    # Return average loss for the input sequence\n",
    "    return loss.data.item() / len(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d069f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall training loop\n",
    "def training_loop(model, optimizer, scheduler, criterion, train_input, test_input):\n",
    "\n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # Training - sample 2000\n",
    "    sampled_train_ids = random.choices(range(train_input.shape[0]), k = 2000)\n",
    "    print(scheduler.get_last_lr())\n",
    "    for i in range(train_input.shape[0]):\n",
    "      sequence = train_input[i, : , :]\n",
    "      input = torch.tensor(sequence[:-1], dtype = torch.long).squeeze().to(device)\n",
    "      target = torch.tensor(sequence[1:], dtype = torch.long).squeeze().to(device)\n",
    "      loss = train_sequence(input, target, model, optimizer, criterion)\n",
    "      running_loss += loss\n",
    "\n",
    "    train_epoch_loss = running_loss / 2000\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    running_loss = 0\n",
    "    # model.eval()\n",
    "    # # Testing\n",
    "    # for i in range(test_input.shape[0]):\n",
    "    #   sequence = test_input[i, : , :]\n",
    "    #   input = torch.tensor(sequence[:-1], dtype = torch.long).squeeze().to(device)\n",
    "    #   target = torch.tensor(sequence[1:], dtype = torch.long).squeeze().to(device)\n",
    "    #   loss = test_sequence(input, target, model, criterion)\n",
    "    #   running_loss += loss\n",
    "\n",
    "    # test_epoch_loss = running_loss / 1000\n",
    "    # test_losses.append(test_epoch_loss)\n",
    "    test_epoch_loss = 0\n",
    "\n",
    "    print('Epoch {}, Train Loss: {}, Test Loss: {}, Time: {}'.format(epoch, train_epoch_loss, test_epoch_loss, datetime.now()))\n",
    "\n",
    "  return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "931dfd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.002]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mLambdaLR(optimizer, lr_lambda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m epoch: lr_lambda \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m epoch)\n\u001b[0;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 12\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[115], line 18\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, optimizer, scheduler, criterion, train_input, test_input)\u001b[0m\n\u001b[0;32m     16\u001b[0m   \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(sequence[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m   target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(sequence[\u001b[38;5;241m1\u001b[39m:], dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 18\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m   running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     21\u001b[0m train_epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2000\u001b[39m\n",
      "Cell \u001b[1;32mIn[114], line 14\u001b[0m, in \u001b[0;36mtrain_sequence\u001b[1;34m(input, target, model, optimizer, criterion)\u001b[0m\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(output, target[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Backpropagate, clip gradient and optimize\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m grad_clipping(model, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\parsh\\Documents\\Parshwa\\UNCC\\1st-Sem\\ML\\MusicGen\\env\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\parsh\\Documents\\Parshwa\\UNCC\\1st-Sem\\ML\\MusicGen\\env\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_pitches = len(note_to_int)\n",
    "hidden_size = 96\n",
    "n_layers = 2\n",
    "n_epochs = 40\n",
    "lr = 0.002\n",
    "lr_lambda = 0.99\n",
    "\n",
    "model = GenerationRNN(input_size = n_pitches, hidden_size = hidden_size, output_size = n_pitches, n_layers = n_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_lambda ** epoch)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_losses, test_losses = training_loop(model, optimizer, scheduler, criterion, train_input, train_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
